---
title: "BI LAB 7"
author: "<Natasha Gichira, Emmanuel Agre, Ryan Gitonga,Fredrick Koech, Dennis Muriuki>"
date: "<31/10/23>"
output:
  github_document: 
    toc: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    df_print: default
editor_options:
  chunk_output_type: console
---

# Student Details

|                                              |     |
|----------------------------------------------|-----|
| **Student ID Number**                        | 124255,120415,124562,118211,124422 |
| **Student Name**                             | Natasha Gichira, Emmanuel Agre, Ryan Gitonga,Fredrick Koech, Dennis Muriuki |
| **BBIT 4.2 Group**                           | C |
| **BI Project Group Name/ID (if applicable)** | Challengers |

# Setup Chunk

**Note:** the following KnitR options have been set as the global defaults: <BR> `knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, collapse = FALSE, tidy = TRUE)`.

More KnitR options are documented here <https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html> and here <https://yihui.org/knitr/options/>.

```{r setup, include=FALSE}
library(formatR)
knitr::opts_chunk$set(
  warning = FALSE,
  collapse = FALSE
)
```

# STEP 1. Install and Load the Required Packages ----
## readr ----
if (require("readr")) {
  require("readr")
} else {
  install.packages("readr", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## naniar ----
if (require("naniar")) {
  require("naniar")
} else {
  install.packages("naniar", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## ggplot2 ----
if (require("ggplot2")) {
  require("ggplot2")
} else {
  install.packages("ggplot2", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## corrplot ----
if (require("corrplot")) {
  require("corrplot")
} else {
  install.packages("corrplot", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## ggcorrplot ----
if (require("ggcorrplot")) {
  require("ggcorrplot")
} else {
  install.packages("ggcorrplot", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# STEP 2. Load the Dataset ----
# Source: http://insideairbnb.com/cape-town/
# Save the dataset as "listings_summary_cape_town.csv" inside the data folder

# License: https://creativecommons.org/licenses/by/4.0/

# Data dictionary (metadata):
# https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=1322284596 # nolint

# Assumptions (Disclaimers): http://insideairbnb.com/data-assumptions/

library(readr)
ccgen <- read_csv("data/ccgen.csv")
View(ccgen)

ccgen <-
  read_csv("data/ccgen.csv",
           col_types =
             cols(CUST_ID = col_character(),
                  BALANCE = col_double(),
                  BALANCE_FREQUENCY = col_character(),
                  PURCHASES = col_double(),
                  ONEOFF_PURCHASES = col_character(),
                  INSTALLMENTS_PURCHASES = col_character(),
                  CASH_ADVANCE = col_double(),
                  PURCHASES_FREQUENCY = col_double(),
                  ONEOFF_PURCHASES_FREQUENCY = col_character(),
                  PURCHASES_INSTALLMENTS_FREQUENCY = col_character(),
                  CASH_ADVANCE_TRX = col_integer()(),
                  PURCHASES_TRX = col_integer(),
                  CREDIT_LIMIT = col_integer(),
                  PAYMENTS = col_double(),
                  MINIMUM_PAYMENT = col_double(),
                  PRC_FULL_PAYMENT = col_double(),
                  TENURE = col_integer(),
                 ))

ccgen$PURCHASES_FREQUENCY <- factor(ccgen$PURCHASES_FREQUENCY)

str(ccgen)
dim(ccgen)
head(ccgen)
summary(ccgen)

# STEP 3. Check for Missing Data and Address it ----
# Are there missing values in the dataset?
any_na(ccgen)

# How many?
n_miss(ccgen)

# What is the proportion of missing data in the entire dataset?
prop_miss(ccgen)

# What is the number and percentage of missing values grouped by
# each variable?
miss_var_summary(ccgen)

# Which variables contain the most missing values?
gg_miss_var(ccgen)

# Which combinations of variables are missing together?
gg_miss_upset(ccgen)

# Where are missing values located (the shaded regions in the plot)?
vis_miss(ccgen) +
  theme(axis.text.x = element_text(angle = 80))

## OPTION 1: Remove the observations with missing values ----
# We can decide to remove all the observations that have missing values
# as follows:
ccgen_removed_obs <- ccgen %>% filter(complete.cases(.))

# The initial dataset had 21,120 observations and 16 variables
dim(ccgen)

# The filtered dataset has 16,205 observations and 16 variables
dim(ccgen_removed_obs)

# Are there missing values in the dataset?
any_na(ccgen_removed_obs)

# The initial dataset had 21,120 observations and 16 variables
dim(ccgen)

# The filtered dataset has 21,120 observations and 14 variables
dim(ccgen_removed_vars)

# Are there missing values in the dataset?
any_na(ccgen_removed_vars)

## OPTION 3: Perform Data Imputation ----

# CAUTION:
# 1. Avoid Over-imputation:
# Be cautious when imputing dates, especially if it is
# Missing Not at Random (MNAR).
# Over-Imputing can introduce bias into your analysis. For example, if dates
# are missing because of a specific event or condition, imputing dates might
# not accurately represent the data.

# 2. Consider the Business Context:
# Dates often have a significant business or domain context. Imputing dates
# may not always be appropriate, as it might distort the interpretation of
# your data. For example, imputing order dates could lead to incorrect insights
# into seasonality trends.

# library(mice) # nolint
# somewhat_correlated_variables <- quickpred(airbnb_cape_town, mincor = 0.3) # nolint

# airbnb_cape_town_imputed <-
#   mice(airbnb_cape_town, m = 11, method = "pmm",
#        seed = 7, # nolint
#        predictorMatrix = somewhat_correlated_variables)

# The choice left is between OPTION 1 and OPTION 2:
# Considering that the 2 variables had 23.3% missing data each,
# we decide to remove the observations that have the missing data (OPTION 1)
# as opposed to removing the entire variable just because 23.3% of its values
# are missing (OPTION 2).

# STEP 4. Perform EDA and Feature Selection ----
## Compute the correlations between variables ----
# We identify the correlated variables because it is these correlated variables
# that can then be used to identify the clusters.

# Create a correlation matrix
# Option 1: Basic Table
cor(ccgen_removed_obs[, c(3, 6, 7, 9, 10, 11, 13, 14, 15, 16)]) %>%
  View()

# Option 2: Basic Plot
cor(ccgen_removed_obs[, c(3, 6, 7, 9, 10, 11, 13, 14, 15, 16)]) %>%
  corrplot(method = "square")

# Option 3: Fancy Plot using ggplot2
corr_matrix <- cor(ccgen_removed_obs[, c(3, 6, 7, 9, 10, 11, 13, 14,
                                                    15, 16)])

p <- ggplot2::ggplot(data = reshape2::melt(corr_matrix),
                     ggplot2::aes(Var1, Var2, fill = value)) +
  ggplot2::geom_tile() +
  ggplot2::geom_text(ggplot2::aes(label = label_wrap(label, width = 10)),
                     size = 4) +
  ggplot2::theme_minimal() +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))

ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE)


## Plot the scatter plots ----

ggplot(ccgen_removed_obs,
       aes(PURCHASES, CREDIT_LIMIT,
           color = PURCHASES_FREQUENCY,
           shape = PURCHASES_FREQUENCY)) +
  geom_point(alpha = 0.5) +
  xlab("purchase frequency") +
  ylab("credit limit")

# A scatter plot to show the number of reviews against price
# per review year
ggplot(ccgen_removed_obs,
       aes(number_of_reviews, price,
           color = last_review)) +
  geom_point(alpha = 0.5) +
  xlab("Number of Reviews") +
  ylab("Daily Price in Rands")


## Transform the data ----
# The K Means Clustering algorithm performs better when data transformation has
# been applied. This helps to standardize the data making it easier to compare
# multiple variables.

summary(ccgen_removed_obs)
model_of_the_transform <- preProcess(ccgen_removed_obs,
                                     method = c("scale", "center"))
print(model_of_the_transform)
ccgen_removed_obs_std <- predict(model_of_the_transform, # nolint
                                            ccgen_removed_obs)
summary(ccgen_removed_obs_std)
sapply(ccgen_removed_obs_std[, c(3, 6, 7, 9, 10, 11, 13, 14,
                                            15, 16)], sd)

## Select the features to use to create the clusters ----
# OPTION 1: Use all the numeric variables to create the clusters
ccgen_removed_vars <-
  ccgen_removed_obs_std[, c(3, 6, 7, 9, 10, 11, 13, 14,
                                       15, 16)]

# OPTION 2: Use only the most significant variables to create the clusters
# This can be informed by feature selection, or by the business case.

# Suppose that the business case is that we need to know the clusters that
# are related to the number of listings a host owns against the listings'
# popularity (measured by number of reviews).

# We need to find the ideal number of listings to own without negatively
# impacting the popularity of the listing.

ccgen_removed_vars <-
  ccgen_removed_obs_std[, c("purchase frequency",
                                       "credit limit")]

# STEP 5. Create the clusters using the K-Means Clustering Algorithm ----
# We start with a random guess of the number of clusters we need
set.seed(7)
kmeans_cluster <- kmeans(ccgen_removed_vars, centers = 3, nstart = 20)

# We then decide the maximum number of clusters to investigate
n_clusters <- 8

# Initialize total within sum of squares error: wss
wss <- numeric(n_clusters)

set.seed(7)

# Investigate 1 to n possible clusters (where n is the maximum number of 
# clusters that we want to investigate)
for (i in 1:n_clusters) {
  # Use the K Means cluster algorithm to create each cluster
  kmeans_cluster <- kmeans(ccgen_removed_vars, centers = i, nstart = 20)
  # Save the within cluster sum of squares
  wss[i] <- kmeans_cluster$tot.withinss
}

## Plot a scree plot ----
# The scree plot should help you to note when additional clusters do not make
# any significant difference (the plateau).
wss_df <- tibble(clusters = 1:n_clusters, wss = wss)

scree_plot <- ggplot(wss_df, aes(x = clusters, y = wss, group = 1)) +
  geom_point(size = 4) +
  geom_line() +
  scale_x_continuous(breaks = c(2, 4, 6, 8)) +
  xlab("Number of Clusters")

scree_plot

# We can add guides to make it easier to identify the plateau (or "elbow").
scree_plot +
  geom_hline(
    yintercept = wss,
    linetype = "dashed",
    col = c(rep("#000000", 5), "#FF0000", rep("#000000", 2))
  )

# The plateau is reached at 6 clusters.
# We therefore create the final cluster with 6 clusters
# (not the initial 3 used at the beginning of this STEP.)
k <- 6
set.seed(7)
# Build model with k clusters: kmeans_cluster
kmeans_cluster <- kmeans(ccgen_removed_vars, centers = k, nstart = 20)

# STEP 6. Add the cluster number as a label for each observation ----
airbnb_cape_town_removed_obs$cluster_id <- factor(kmeans_cluster$cluster)

## View the results by plotting scatter plots with the labelled cluster ----
ggplot(ccgen_removed_obs, aes(PURCHASES, CREDIT_LIMIT,
                                         color = cluster_id)) +
  geom_point(alpha = 0.5) +
  xlab("purchases") +
  ylab("credit limit")
